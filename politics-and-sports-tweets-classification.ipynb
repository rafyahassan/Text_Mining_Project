{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8896,"databundleVersionId":110042,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Load data**","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:21.899495Z","iopub.execute_input":"2024-02-07T17:47:21.899901Z","iopub.status.idle":"2024-02-07T17:47:21.905535Z","shell.execute_reply.started":"2024-02-07T17:47:21.899871Z","shell.execute_reply":"2024-02-07T17:47:21.904642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/deeptweets/train.csv')\ntest_data = pd.read_csv('/kaggle/input/deeptweets/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:22.183907Z","iopub.execute_input":"2024-02-07T17:47:22.184401Z","iopub.status.idle":"2024-02-07T17:47:22.225468Z","shell.execute_reply.started":"2024-02-07T17:47:22.184361Z","shell.execute_reply":"2024-02-07T17:47:22.224124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explore the first lines**","metadata":{}},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:22.227617Z","iopub.execute_input":"2024-02-07T17:47:22.228304Z","iopub.status.idle":"2024-02-07T17:47:22.241131Z","shell.execute_reply.started":"2024-02-07T17:47:22.228263Z","shell.execute_reply":"2024-02-07T17:47:22.239688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:22.608108Z","iopub.execute_input":"2024-02-07T17:47:22.608746Z","iopub.status.idle":"2024-02-07T17:47:22.621538Z","shell.execute_reply.started":"2024-02-07T17:47:22.608653Z","shell.execute_reply":"2024-02-07T17:47:22.619836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Descriptive statistics**","metadata":{}},{"cell_type":"code","source":"train_data.describe()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:22.624518Z","iopub.execute_input":"2024-02-07T17:47:22.625966Z","iopub.status.idle":"2024-02-07T17:47:22.642326Z","shell.execute_reply.started":"2024-02-07T17:47:22.625918Z","shell.execute_reply":"2024-02-07T17:47:22.641060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Information on data types**","metadata":{}},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:22.643599Z","iopub.execute_input":"2024-02-07T17:47:22.644032Z","iopub.status.idle":"2024-02-07T17:47:22.660645Z","shell.execute_reply.started":"2024-02-07T17:47:22.644002Z","shell.execute_reply":"2024-02-07T17:47:22.659727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Exploratory data analysis****","metadata":{}},{"cell_type":"markdown","source":"Visualize the two classes in the following bar chart","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(x='Label', data=train_data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:22.662202Z","iopub.execute_input":"2024-02-07T17:47:22.663145Z","iopub.status.idle":"2024-02-07T17:47:22.802961Z","shell.execute_reply.started":"2024-02-07T17:47:22.663108Z","shell.execute_reply":"2024-02-07T17:47:22.801510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"we see that the two classes are balanced","metadata":{}},{"cell_type":"markdown","source":"Identify specific patterns in tweets","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nsport_tweets = train_data[train_data['Label'] == 'Sports']['TweetText'].values\npolitics_tweets = train_data[train_data['Label'] == 'Politics']['TweetText'].values\n\n# WordCloud pour la classe \"Sport\"\nif len(sport_tweets) > 0:\n    wordcloud_sport = WordCloud(width=800, height=400, background_color='white').generate(' '.join(sport_tweets))\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud_sport, interpolation='bilinear')\n    plt.title('WordCloud pour la Classe \"Sport\"')\n    plt.axis('off')\n    plt.show()\nelse:\n    print(\"Aucun tweet dans la classe 'Sport'.\")\n# WordCloud pour la classe \"Politique\"\n# VÃ©rifier si la classe \"Politique\" a au moins un tweet\nif len(politics_tweets) > 0:\n    wordcloud_politics = WordCloud(width=800, height=400, background_color='white').generate(' '.join(politics_tweets))\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud_politics, interpolation='bilinear')\n    plt.title('WordCloud pour la Classe \"Politique\"')\n    plt.axis('off')\n    plt.show()\nelse:\n    print(\"Aucun tweet dans la classe 'Politique'.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:23.257224Z","iopub.execute_input":"2024-02-07T17:47:23.257696Z","iopub.status.idle":"2024-02-07T17:47:26.011322Z","shell.execute_reply.started":"2024-02-07T17:47:23.257638Z","shell.execute_reply":"2024-02-07T17:47:26.009920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing the Data:**","metadata":{}},{"cell_type":"markdown","source":"> 1. Text Cleaning:\n\nEliminate special characters, URLs, and irrelevant symbols from the tweets.\nConvert the text to lowercase to maintain consistency.\n> 2. Tokenization:\n\nTokenize the cleaned text, breaking it into individual words (tokens).\n> 3. Stopword Removal:\n\nRemove common stopwords (e.g., \"the,\" \"and\") to emphasize meaningful words.","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ncustom_stopwords = set([\"the\", \"and\", \"is\", \"in\", \"to\", \"of\", \"it\", \"this\", \"that\"])\n\n# Function for text cleaning\ndef clean_text(text):\n    # Remove special characters and URLs\n    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters and symbols\n    \n    # Convert the text to lowercase\n    text = text.lower()\n    \n    return text\n\n# Function for tokenization and stopword removal\ndef tokenize_and_remove_stopwords(text):\n    # Tokenize the text\n    tokens = nltk.word_tokenize(text)\n    \n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [word for word in tokens if word.lower() not in custom_stopwords]\n    \n    return tokens\n\n# Combine text cleaning and tokenization into a single function\ndef preprocess_text(text):\n    cleaned_text = clean_text(text)\n    tokens = tokenize_and_remove_stopwords(cleaned_text)\n    return ' '.join(tokens)\n\n# Apply preprocessing to each tweet in the dataset\ntrain_data['ProcessedText'] = train_data['TweetText'].apply(preprocess_text)\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(train_data['ProcessedText'], train_data['Label'], test_size=0.2, random_state=42)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:26.014277Z","iopub.execute_input":"2024-02-07T17:47:26.014759Z","iopub.status.idle":"2024-02-07T17:47:28.139929Z","shell.execute_reply.started":"2024-02-07T17:47:26.014718Z","shell.execute_reply":"2024-02-07T17:47:28.139118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Assuming you have 'ProcessedText' column in your datase\n\n# WordCloud for the \"Sports\" class\nsports_tweets = train_data[train_data['Label'] == 'Sports']['ProcessedText'].values\nif len(sports_tweets) > 0:\n    wordcloud_sports = WordCloud(width=800, height=400, background_color='white').generate(' '.join(sports_tweets))\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud_sports, interpolation='bilinear')\n    plt.title('WordCloud for Class \"Sports after preprocissing \"')\n    plt.axis('off')\n    plt.show()\nelse:\n    print(\"No tweets in the 'Sports' class.\")\n\n# WordCloud for the \"Politics\" class\npolitics_tweets = train_data[train_data['Label'] == 'Politics']['ProcessedText'].values\nif len(politics_tweets) > 0:\n    wordcloud_politics = WordCloud(width=800, height=400, background_color='white').generate(' '.join(politics_tweets))\n    plt.figure(figsize=(10, 5))\n    plt.imshow(wordcloud_politics, interpolation='bilinear')\n    plt.title('WordCloud for Class \"Politics\" after preprocissing ')\n    plt.axis('off')\n    plt.show()\nelse:\n    print(\"No tweets in the 'Politics' class.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:28.141194Z","iopub.execute_input":"2024-02-07T17:47:28.143089Z","iopub.status.idle":"2024-02-07T17:47:31.081954Z","shell.execute_reply.started":"2024-02-07T17:47:28.143044Z","shell.execute_reply":"2024-02-07T17:47:31.080397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:31.085358Z","iopub.execute_input":"2024-02-07T17:47:31.085825Z","iopub.status.idle":"2024-02-07T17:47:31.102881Z","shell.execute_reply.started":"2024-02-07T17:47:31.085781Z","shell.execute_reply":"2024-02-07T17:47:31.101129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Feature extraction**","metadata":{}},{"cell_type":"markdown","source":"For feature extraction, I'll show you how to use both TF-IDF to represent tweets as numerical vectors.","metadata":{}},{"cell_type":"markdown","source":" Using TF-IDF(Term Frequency-Inverse Document Frequency):","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Vectorize the text using TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features based on your dataset\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_val_tfidf = tfidf_vectorizer.transform(X_val)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:31.104750Z","iopub.execute_input":"2024-02-07T17:47:31.105202Z","iopub.status.idle":"2024-02-07T17:47:31.240699Z","shell.execute_reply.started":"2024-02-07T17:47:31.105161Z","shell.execute_reply":"2024-02-07T17:47:31.239400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"****Model Training****","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Initialize the SVM model\nsvm_model = SVC(kernel='linear', random_state=42)  # You can experiment with different kernels\n\n# Train the SVM model using TF-IDF features\nsvm_model.fit(X_train_tfidf, y_train)\n\n# Make predictions on the validation set\nsvm_predictions = svm_model.predict(X_val_tfidf)\n\n# Evaluate the SVM model\nsvm_accuracy = accuracy_score(y_val, svm_predictions)\nsvm_classification_rep = classification_report(y_val, svm_predictions)\n\nprint(f\"SVM Accuracy: {svm_accuracy:.2f}\")\nprint(\"SVM Classification Report:\\n\", svm_classification_rep)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:31.242144Z","iopub.execute_input":"2024-02-07T17:47:31.244321Z","iopub.status.idle":"2024-02-07T17:47:33.473621Z","shell.execute_reply.started":"2024-02-07T17:47:31.244284Z","shell.execute_reply":"2024-02-07T17:47:33.472764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Hyperparameter tuning**","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter tuning is a crucial step in optimizing the performance of my machine learning model. For SVM, one common hyperparameter to tune is the regularization parameter **C**. The choice of the kernel (**linear**, **rbf**, **poly**, etc.) and other parameters such as the degree of the polynomial **kernel** or the **gamma** parameter can also impact the model's performance.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nparam_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf', 'poly'], 'degree': [2, 3, 4], 'gamma': ['scale', 'auto']}\n\nsvm_model = SVC(random_state=42)\n\n# Create the GridSearchCV object\nrandom_search = RandomizedSearchCV(svm_model, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n\n# Perform the randomized search on the training data\nrandom_search.fit(X_train_tfidf, y_train)\n\n# Print the best hyperparameters\nprint(\"Best Hyperparameters:\", random_search.best_params_)\n\n# Get the best model from the randomized search\nbest_svm_model = random_search.best_estimator_\n\n# Make predictions on the validation set using the best model\nbest_predictions = best_svm_model.predict(X_val_tfidf)\n\n# Evaluate the best model\nbest_accuracy = accuracy_score(y_val, best_predictions)\nbest_classification_rep = classification_report(y_val, best_predictions)\n\nprint(f\"Best SVM Accuracy: {best_accuracy:.2f}\")\nprint(\"Best SVM Classification Report:\\n\", best_classification_rep)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:47:33.474778Z","iopub.execute_input":"2024-02-07T17:47:33.475765Z","iopub.status.idle":"2024-02-07T17:48:22.919572Z","shell.execute_reply.started":"2024-02-07T17:47:33.475731Z","shell.execute_reply":"2024-02-07T17:48:22.917737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Performance of the model**","metadata":{}},{"cell_type":"markdown","source":"break down performance metrics into different stages of the process, including feature extraction, model learning and output prediction","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Vectorisation des textes avec TF-IDF\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Ajustez max_features en fonction de votre ensemble de donnÃ©es\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_val_tfidf = tfidf_vectorizer.transform(X_val)\n\n# Mesurer le temps d'exÃ©cution pour l'apprentissage du modÃ¨le SVM avec TF-IDF\nstart_time = time.time()\n# Initialiser le modÃ¨le SVM\nsvm_model = SVC(kernel='linear', random_state=42)\n# EntraÃ®ner le modÃ¨le SVM avec TF-IDF\nsvm_model.fit(X_train_tfidf, y_train)\nend_time = time.time()\nmodel_training_time = end_time - start_time\n\n# Mesurer le temps d'exÃ©cution pour la prÃ©diction de la sortie avec TF-IDF\nstart_time = time.time()\n# Faire des prÃ©dictions sur l'ensemble de validation avec TF-IDF\nsvm_predictions = svm_model.predict(X_val_tfidf)\nend_time = time.time()\noutput_prediction_time = end_time - start_time\n\n# Ãvaluer la performance du modÃ¨le avec TF-IDF\naccuracy = accuracy_score(y_val, svm_predictions)\nclassification_rep = classification_report(y_val, svm_predictions)\n\n# Afficher les rÃ©sultats avec les temps d'exÃ©cution\nprint(f\"Model Training Time: {model_training_time:.4f} seconds\")\nprint(f\"Output Prediction Time: {output_prediction_time:.4f} seconds\")\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(\"Classification Report:\\n\", classification_rep)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:48:22.921380Z","iopub.execute_input":"2024-02-07T17:48:22.921750Z","iopub.status.idle":"2024-02-07T17:48:25.318904Z","shell.execute_reply.started":"2024-02-07T17:48:22.921716Z","shell.execute_reply":"2024-02-07T17:48:25.317756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test the model**","metadata":{}},{"cell_type":"markdown","source":"To test my  model with the test data, you can follow a similar procedure to the one we used for the validation data","metadata":{}},{"cell_type":"code","source":"# DÃ©finir les mÃªmes fonctions de prÃ©traitement que pour les donnÃ©es d'entraÃ®nement\ncustom_stopwords = set([\"the\", \"and\", \"is\", \"in\", \"to\", \"of\", \"it\", \"this\", \"that\"])\n\ndef clean_text(text):\n    text = re.sub(r'http\\S+', '', text)  # Supprimer les URLs\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Supprimer les caractÃ¨res non alphabÃ©tiques et les symboles\n    text = text.lower()  # Convertir le texte en minuscules\n    return text\n\ndef tokenize_and_remove_stopwords(text):\n    tokens = nltk.word_tokenize(text)\n    tokens = [word for word in tokens if word.lower() not in custom_stopwords]\n    return tokens\n\ndef preprocess_text(text):\n    cleaned_text = clean_text(text)\n    tokens = tokenize_and_remove_stopwords(cleaned_text)\n    return ' '.join(tokens)\n\n# Data processing\ntest_data['ProcessedText'] = test_data['TweetText'].apply(preprocess_text)\n\nX_test_tfidf = tfidf_vectorizer.transform(test_data['ProcessedText'])\n\nsvm_predictions_test = svm_model.predict(X_test_tfidf)\n\nsvm_predictions_test = svm_model.predict(X_test_tfidf)\n\ntest_data['Predicted_Label'] = svm_predictions_test\n#print test Data after preprocessing\ntest_data.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:49:31.328329Z","iopub.execute_input":"2024-02-07T17:49:31.328775Z","iopub.status.idle":"2024-02-07T17:49:32.876116Z","shell.execute_reply.started":"2024-02-07T17:49:31.328745Z","shell.execute_reply":"2024-02-07T17:49:32.875291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:49:40.161966Z","iopub.execute_input":"2024-02-07T17:49:40.162343Z","iopub.status.idle":"2024-02-07T17:49:40.175526Z","shell.execute_reply.started":"2024-02-07T17:49:40.162318Z","shell.execute_reply":"2024-02-07T17:49:40.174132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df = test_data[['TweetId', 'Predicted_Label']]\nresult_df.to_csv('Resultat.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T17:50:10.319249Z","iopub.execute_input":"2024-02-07T17:50:10.319881Z","iopub.status.idle":"2024-02-07T17:50:10.332518Z","shell.execute_reply.started":"2024-02-07T17:50:10.319832Z","shell.execute_reply":"2024-02-07T17:50:10.331003Z"},"trusted":true},"execution_count":null,"outputs":[]}]}